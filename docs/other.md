# 其他奇怪的问题

我或其他人看到的一些情况（轶事证据）：

- 使用存储 16 位浮点权重的 mlmodel 文件似乎比使用 32 位浮点的模型更慢。在 GPU 上这没有区别，因为它总是将权重转换为 16 位浮点，但神经引擎似乎在这方面有困难。（我不是 100%确定这里发生了什么。）
- A12 及更高版本的处理器有一个"智能计算系统"，用于确定任务应该在 CPU、GPU 还是 ANE 上运行。因此，即使模型可以在 ANE 上无问题地运行，Core ML 仍可能决定在其他地方运行它，这取决于系统正在做什么。来自[Twitter](https://twitter.com/eugenebokhan/status/1251423554861752320?s=20)："我们还发现，如果 NPU 支持模型的所有层且 GPU 处于休息状态，那么 CoreML 很可能会在 GPU 上运行它。但如果 GPU 负载工作繁重，你可以期望模型在 NPU 上运行。"
- 即使你的模型中的所有层都与 ANE 兼容，Core ML 仍可能决定在 GPU 上运行你的模型。如果你认为发生了这种情况，将你的模型分成两部分，看看第一部分会发生什么。它现在在 ANE 上运行了吗？我见过这种情况：一旦模型大小超过某个阈值，Core ML 可能会认为 GPU 更合适，即使该模型完全可以在 ANE 上运行。
